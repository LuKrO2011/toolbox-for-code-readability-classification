= Toolbox: Code Readability Classification

This toolbox served various purposes in the context of readability classification of Java source code snippets:

1. Work on previous datasets:
- Unification of readability classification datasets.
- Combination of the datasets into the https://huggingface.co/datasets/se2p/code-readability-merged[merged] dataset.

2. Creation of the mined-and-modified dataset:
- Mining of repositories for the mined-and-modified dataset.
- Extraction of methods from Java files.
- Removal of comments from Java method code snippets.
- Publishing of the https://huggingface.co/datasets/se2p/code-readability-krod[mined-and-modified] dataset.

3. Sampling for a survey:
- Execution of Stratified Sampling based on code features.
- Extraction of the differences between original and modified code snippets.
- Creation of questionnaires for the mined-and-modified dataset.

4. Plotting of evaluation results

All modifications other than comments removal are done using the https://github.com/LuKrO2011/readability-decreasing-heuristics[RDH/REDEC] tool.
The https://github.com/LuKrO2011/readability-classifier[model] itself is not part of this repository either.
For executing checkstyle we further used parts of https://github.com/sphrilix/styler2.0[Styler 2.0].

You can find a visualization of the usage of the subcommands in the following diagram:
image:script-overview.svg[alt="Script Overview"]


== Table of Contents

* <<Installation>>
* <<Usage>>
** <<Stratified_Sampling>>
** <<Extract_sampled>>
** <<Extract_files>>
** <<Extract_methods>>
** <<Convert_datasets>>
*** <<Convert_datasets_csv>>
*** <<Convert_datasets_two_folders>>
** <<Combine_datasets>>
** <<Download_datasets>>
** <<Upload_datasets>>
** <<Craft_surveys>>
** <<Extract_diff>>
** <<Remove_comments>>
** <<Others>>
* <<Podman>>

[[Installation]]
== Installation

To set up the project and its dependencies, follow these steps:

Clone this repository to your local machine:

[source,bash]
----
git clone https://github.com/LuKrO2011/readability-preprocessing
cd readability-preprocessing
----

Install Poetry if you haven't already:

[source,bash]
----
pip install poetry
----

Create a virtual environment and install the project's dependencies using Poetry:

[source,bash]
----
poetry install
----

Activate the virtual environment:

[source,bash]
----
poetry shell
----

For Developers: Activate the pre-commit hooks:

----
pre-commit install
----

If you want to use the <<Stratified_Sampling>> command, you additionally need to install https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html[Java 17].

Now you're ready to use the source code readability preprocessing tool.

[[Usage]]
== Usage

In this section you will find information on how to use the source code readability preprocessing tool.
Use `-h` or `--help` to get help on the usage of the tool or a subcommand.

[[Feature_Extraction]]
=== Feature Extraction

To extract features from Java source code snippets, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py FEATURES [-h] --input INPUT --output OUTPUT
----

* `--input` or `-i` is the path to the dictionary with the Java source code snippets.
* `--output` or `-o` is the path to the file where the `features.csv` will be saved.

Example:
----
python src/readability_preprocessing/main.py FEATURES -i <input_path> -o <output_path>
----


[[Stratified_Sampling]]
=== Stratified Sampling

To perform Stratified Sampling based on code features, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py SAMPLE --input INPUT [--save SAVE] [--num-stratas NUM_STRATAS] [--snippets-per-stratum SNIPPETS_PER_STRATUM]
----

* `--input` or `-i` is the path to the dictionary with the Java source code snippets or to a csv file containing the paths and features of the java files.
* `--save` or `-s` is the path to the file where the sampled snippets and the features (as csv) will be saved.
* `--num-stratas` or `-n` is the number of stratas to sample from.
* `--snippets-per-stratum` or `-sps` is the number of snippets to sample from each stratum.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py SAMPLE -i <input_path> -s <save_path>
----

[[Extract_sampled]]
=== Extract Sampled

To extract the sampled snippets from a dictionary with the Java source code snippets, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py [-h] --input INPUT [INPUT ...] --sampling SAMPLING --output OUTPUT
----

* `--input` or `-i` is the path to the dictionary with the Java source code snippets.
* `--sampling` or `-s` is the path to the file containing the sampling information of <<Stratified_Sampling>>.
* `--output` or `-o` is the path to the directory where the extracted sampled snippets will be saved.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py -i <input_path> -s <sampling_path> -o <output_path>
----

[[Extract_files]]
=== Extract Files

To extract Java source code files from multiple directories, use the following command:

[source,bash]
----
main.py EXTRACT_FILES [-h] --input INPUT --output OUTPUT [--non-violated-subdir NON_VIOLATED_SUBDIR]
----

* `--input` or `-i` is the path to the directory containing the directories with the Java source code files.
* `--output` or `-o` is the path to the directory where the extracted Java source code files will be saved.
* `--non-violated-subdir` or `-nvs` is the name of the subdirectory where the non-violated Java source code files are saved.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_FILES -i <input_path> -o <output_path>
----

[[Extract_methods]]
=== Extract Methods

To extract methods from a dictionary of Java source code snippets, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_METHODS --input INPUT --output OUTPUT [--overwrite-mode {OverwriteMode.OVERWRITE,OverwriteMode.SKIP}] [--include-method-comments INCLUDE_METHOD_COMMENTS] [--comments-required COMMENTS_REQUIRED] [--remove-indentation REMOVE_INDENTATION]
----

* `--input` or `-i` is the path to the dictionary with the Java source code snippets.
* `--output` or `-o` is the path to the directory where the extracted methods will be saved.
* `--overwrite-mode` or `-om` is the overwrite mode to use.
The default is `OverwriteMode.SKIP`.
* `--include-method-comments` or `-imc` is a boolean flag indicating whether to include method comments in the extracted methods.
The default is `True`.
* `--comments-required` or `-cr` is a boolean flag indicating whether to require comments for extracted methods.
The default is `True`.
* `--remove-indentation` or `-ri` is a boolean flag indicating whether to remove indentation from the extracted methods.
The default is `True`.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_METHODS -i <input_path> -o <output_path>
----

[[Extract_diff]]
=== Extract Diff

To extract the differences between original and modified code snippets, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_DIFF [-h] --input INPUT [--output OUTPUT] [--methods-dir-name METHODS_DIR_NAME]
----

* `--input` or `-i` is the path to the folder containing the stratas (with rdhs and methods).
* `--output` or `-o` is the path to the directory where the extracted differences will be saved.
* `--methods-dir-name` or `-mdn` is the name of the directory containing the original methods.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py EXTRACT_DIFF -i <input_path> -o <output_path>
----

[[Convert_datasets]]
=== Convert Dataset

This tool supports converting datasets from csv and a dictionary of Java source code snippets to a https://huggingface.co/[Hugging Face] dataset.
To do this, see <<Convert_datasets_csv>>.
You can also convert two folders containing Java source code files, one folder with readable and the other with non-readable Java source code files, to a HuggingFace dataset.
To do this, see <<Convert_datasets_two_folders>>.

[[Convert_datasets_csv]]
==== Convert datasets csv

To convert a csv file to a HuggingFace dataset, use the following command:

[source,bash]
----
src/readability_preprocessing/main.py CONVERT_CSV [-h] --input INPUT --csv CSV --output OUTPUT --dataset-type {SCALABRIO,BW,DORN}
----

* `--input` or `-i` is the path to the directory containing the directories with the Java source code files.
* `--csv` or `-c` is the path to the csv file containing the paths and features of the java files.
* `--output` or `-o` is the path to the directory where the converted dataset will be saved.
* `--dataset-type` or `-dt` is the type of the dataset to convert.
Currently, the following types are supported: `SCALABRIO`, `BW`, `DORN`.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py CONVERT_CSV -i <input_path> -c <csv_path> -o <output_path> -dt SCALABRIO
----

[[Convert_datasets_two_folders]]
==== Convert dataset from two folders

To convert two folders containing Java source code files, one folder with readable and the other with non-readable Java source code files, to a HuggingFace dataset, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py CONVERT_TWO_FOLDERS [-h] --readable READABLE --not-readable NOT_READABLE --output OUTPUT [--readable-score READABLE_SCORE] [--not-readable-score NOT_READABLE_SCORE]
----

* `--readable` or `-r` is the path to the directory containing the readable Java source code files.
* `--not-readable` or `-nr` is the path to the directory containing the non-readable Java source code files.
* `--output` or `-o` is the path to the directory where the converted dataset will be saved.
* `--readable-score` or `-rs` is the score to assign to the readable Java source code files.
The default is `4.5`.
* `--not-readable-score` or `-nrs` is the score to assign to the non-readable Java source code files.
The default is `1.5`.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py CONVERT_TWO_FOLDERS -r <readable_path> -nr <not_readable_path> -o <output_path>
----

[[Combine_datasets]]
=== Combine Dataset

To combine multiple HuggingFace datasets into one, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py COMBINE [-h] --input INPUT [INPUT ...] --output OUTPUT [--percent-to-remove PERCENT_TO_REMOVE]
----

* `--input` or `-i` is the paths to the directories containing the HuggingFace datasets.
* `--output` or `-o` is the path to the directory where the combined dataset will be saved.
* `--percent-to-remove` or `-ptr` is the percentage of examples to remove from the combined dataset.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py COMBINE -i <input_path_1> <input_path_2> -o <output_path>
----

[[Download_datasets]]
=== Download dataset

To download a dataset from the HuggingFace Hub, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py DOWNLOAD [-h] --name NAME --output OUTPUT [--token-file TOKEN_FILE]
----

* `--name` or `-n` is the name of the dataset to download.
* `--output` or `-o` is the path to the directory where the downloaded dataset will be saved.
* `--token-file` or `-tf` is the path to the file containing the HuggingFace API token.
If not provided, the dataset must be public.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py DOWNLOAD -n <dataset_name> -o <output_path>
----

[[Upload_datasets]]
=== Upload dataset

To upload a dataset to the HuggingFace Hub, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py UPLOAD [-h] --input INPUT --name NAME --token-file TOKEN_FILE
----

* `--input` or `-i` is the path to the directory containing the dataset to upload.
* `--name` or `-n` is the name of the dataset to upload.
* `--token-file` or `-tf` is the path to the file containing the HuggingFace API token.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py UPLOAD -i <input_path> -n <dataset_name> -tf <token_file_path>
----

[[Craft_surveys]]
=== Craft Surveys

To craft questionnaires (= survey sheets) for a survey, use the following command:

[source,bash]
----
usage: main.py CRAFT_SURVEYS [-h] --input INPUT --output OUTPUT
                             [--snippets-per-sheet SNIPPETS_PER_SHEET]
                             [--num-sheets NUM_SHEETS]
                             [--sample-amount-path SAMPLE_AMOUNT_PATH]
                             [--original-name ORIGINAL_NAME]
                             [--nomod-name NOMOD_NAME]
                             [--exclude-path EXCLUDE_PATH]
----

* `--input` or `-i`: Path to the directory containing the dataset or samples for which surveys will be crafted.
* `-output` or `-o`: Path to the directory where the crafted surveys will be saved.
* `--snippets-per-sheet`: Number of snippets to include per survey sheet.
* `--num-sheets`: Number of survey sheets to generate.
* `--sample-amount-path`: Path to a file containing the amount of samples for each group.
* `--original-name`: Name of the group containing the original samples.
* `--nomod-name`: Name of the group containing the just-pretty-print/not modified samples.
* `--exclude-path`: Path to a file containing a list of file paths to exclude from the surveys.

Example:

----
python src/readability_preprocessing/main.py CRAFT_SURVEYS -i <input_path> -o <output_path> --original-name "original" --nomod-name "just-pretty-print"
----

[[Remove_comments]]
=== Remove Comments

To remove comments from Java method code snippets, use the following command:

[source,bash]
----
python src/readability_preprocessing/main.py REMOVE_COMMENTS [-h] --input INPUT --output OUTPUT [--probability PROBABILITY]
----

* `--input` or `-i` is the path to the dictionary with the Java method code snippets.
* `--output` or `-o` is the path to the directory where the Java method code snippets without comments will be saved.
* `--probability` or `-p` is the probability of removing comments from the Java method code snippets.

Example:

[source,bash]
----
python src/readability_preprocessing/main.py REMOVE_COMMENTS -i <input_path> -o <output_path>
----

[[Others]]
=== Others

There are various other scripts that can be executed by executing the file itself.
For usage and extension of these scripts we recommend to use the functions in the `prolific` package over the scripts of the `evaluation` package. Later are kept to reproduce the results of the master thesis.

[[Podman]]
== Podman

To build the podman container, run the following command:

[source,bash]
----
podman build -t readability-preprocessing .
----

- t : name of the container
- . : path to the Dockerfile

To run the podman container, run the following command:

[source,bash]
----
podman run -it --rm -v $(pwd):/app readability-preprocessing
----

- it : interactive mode
- rm : remove container after exit
- v $(pwd):/app : mount current directory to /app in container
- readability-classifier : name of the container
